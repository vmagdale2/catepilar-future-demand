{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "3oJpqB2XjLIi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yqlxjj9uiR1o"
      },
      "outputs": [],
      "source": [
        "test_df = pd.read_csv('test_df_MR_lite.csv')\n",
        "train_df = pd.read_csv('train_df_lite_withsolution.csv')\n",
        "valid_df = pd.read_csv('valid_with_solution_MR.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FfyCxXEiXZF",
        "outputId": "df689612-08ee-4510-b4ad-b54f154dc738"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['fiModelDescriptor_freq', 'ProductSize_Large / Medium',\n",
              "       'ProductGroup_SSL', 'Ripper_Yes', 'ProductSize_Medium',\n",
              "       'ProductSize_Large', 'Enclosure_EROPS w AC', 'UnemploymentRate',\n",
              "       'fiSecondaryDesc_freq', 'Estimated_Hours_log'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD8Dr_NmjYNP",
        "outputId": "7621e18a-5bd1-4f80-8ef7-6f8e52d8fbc8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12457, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYm9tfbJjTh-",
        "outputId": "812e1261-4bfd-470a-d65a-cb2c67251a86"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['fiModelDescriptor_freq', 'ProductGroup_SSL', 'ProductSize_Large',\n",
              "       'Enclosure_EROPS w AC', 'UnemploymentRate',\n",
              "       'ProductSize_Large / Medium', 'SalePrice', 'fiSecondaryDesc_freq',\n",
              "       'Estimated_Hours_log', 'Ripper_Yes', 'ProductSize_Medium',\n",
              "       'Drive_System_Four Wheel Drive'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8eBftKBjaDi",
        "outputId": "9320fe7b-9e22-4fb2-f1eb-b7f0039aa3e3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(421246, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUdk4Kq-jVN2",
        "outputId": "4f9154e2-a9d9-4519-9c95-68f77f9e0ade"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['SalesID', 'fiModelDescriptor_freq', 'ProductSize_Large / Medium',\n",
              "       'ProductGroup_SSL', 'Ripper_Yes', 'ProductSize_Medium',\n",
              "       'ProductSize_Large', 'Enclosure_EROPS w AC', 'UnemploymentRate',\n",
              "       'fiSecondaryDesc_freq', 'Estimated_Hours_log', 'SalePrice'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTYCPMC4jWUp",
        "outputId": "aeab421b-6c04-4564-f7fa-59849b5af437"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11573, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2Xj20pCs_mF",
        "outputId": "bcc04ae0-1da7-47fa-9174-2f75474a64c9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "checkpoint_dir = '/content/drive/MyDrive/demand_prediction_checkpoints/'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "6FG8eZletEFs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import json\n",
        "from datetime import datetime\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import torch\n",
        "print(f\"PyTorch GPU Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R39_ggQFtOsv",
        "outputId": "595073fb-c909-4f4a-b8de-3b82c91b3d88"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch GPU Available: True\n",
            "GPU Name: NVIDIA L4\n",
            "GPU Memory: 22.16 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(y_true, y_pred, model_name):\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    mask = y_true != 0\n",
        "    mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
        "\n",
        "    return {\n",
        "        'model': model_name,\n",
        "        'rmse': rmse,\n",
        "        'mae': mae,\n",
        "        'r2': r2,\n",
        "        'mape': mape\n",
        "    }\n",
        "\n",
        "def save_checkpoint(data, filename):\n",
        "    filepath = os.path.join(checkpoint_dir, filename)\n",
        "    with open(filepath, 'wb') as f:\n",
        "        pickle.dump(data, f)\n",
        "    print(f\"Checkpoint saved: {filename}\")\n",
        "\n",
        "def load_checkpoint(filename):\n",
        "    filepath = os.path.join(checkpoint_dir, filename)\n",
        "    if os.path.exists(filepath):\n",
        "        with open(filepath, 'rb') as f:\n",
        "            print(f\"Checkpoint loaded: {filename}\")\n",
        "            return pickle.load(f)\n",
        "    return None\n",
        "\n",
        "def print_metrics(metrics):\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"Model: {metrics['model']}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"RMSE: ${metrics['rmse']:,.2f}\")\n",
        "    print(f\"MAE:  ${metrics['mae']:,.2f}\")\n",
        "    print(f\"R²:   {metrics['r2']:.4f}\")\n",
        "    print(f\"MAPE: {metrics['mape']:.2f}%\")\n",
        "    print(f\"{'='*50}\")"
      ],
      "metadata": {
        "id": "Tmwfy2AutYDG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCXGAyOIslja",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "574e6900-9c5c-4fec-fc87-cc705360df2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading training data from CSV...\n",
            "Training data shape: (421246, 11)\n",
            "Memory usage: 10.45 MB\n",
            "Columns: ['fiModelDescriptor_freq', 'ProductGroup_SSL', 'ProductSize_Large', 'Enclosure_EROPS w AC', 'UnemploymentRate', 'ProductSize_Large / Medium', 'SalePrice', 'fiSecondaryDesc_freq', 'Estimated_Hours_log', 'Ripper_Yes', 'ProductSize_Medium']\n"
          ]
        }
      ],
      "source": [
        "processed_data = load_checkpoint('processed_data.pkl')\n",
        "\n",
        "if processed_data is None:\n",
        "    print(\"Loading training data from CSV...\")\n",
        "    dtypes = {\n",
        "        'fiModelDescriptor_freq': 'float32',\n",
        "        'fiSecondaryDesc_freq': 'float32',\n",
        "        'UnemploymentRate': 'float32',\n",
        "        'Estimated_Hours_log': 'float32',\n",
        "        'SalePrice': 'float32'\n",
        "    }\n",
        "    train_df = pd.read_csv('/content/train_df_MR_lite.csv', dtype=dtypes)\n",
        "\n",
        "    print(f\"Training data shape: {train_df.shape}\")\n",
        "    print(f\"Memory usage: {train_df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
        "    print(f\"Columns: {list(train_df.columns)}\")\n",
        "else:\n",
        "    print(\"Loading preprocessed data from checkpoint...\")\n",
        "    train_df = processed_data"
      ],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": [
        "# --run next--\n",
        "if processed_data is None:\n",
        "    print(train_df.isnull().sum())\n",
        "    print(f\"Total missing values: {train_df.isnull().sum().sum()}\")\n",
        "\n",
        "    if train_df.isnull().sum().sum() > 0:\n",
        "        numeric_cols = train_df.select_dtypes(include=[np.number]).columns\n",
        "        for col in numeric_cols:\n",
        "            if train_df[col].isnull().sum() > 0:\n",
        "                median_value = train_df[col].median()\n",
        "                train_df[col].fillna(median_value, inplace=True)\n",
        "                print(f\"  Filled {col} with median: {median_value}\")\n",
        "\n",
        "        print(f\"Missing values after handling: {train_df.isnull().sum().sum()}\")\n",
        "        save_checkpoint(train_df, 'processed_data_imputed.pkl')\n",
        "else:\n",
        "    print(\"Data was loaded.\")\n",
        "\n",
        "split_data = load_checkpoint('train_test_split.pkl')\n",
        "\n",
        "if split_data is None:\n",
        "    print(\"Creating train-test split...\")\n",
        "\n",
        "    X = train_df.drop('SalePrice', axis=1)\n",
        "    y = train_df['SalePrice']\n",
        "\n",
        "    print(f\"Features shape: {X.shape}\")\n",
        "    print(f\"Target shape: {y.shape}\")\n",
        "    print(f\"Target range: ${y.min():,.0f} - ${y.max():,.0f}\")\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"Train set: {X_train.shape}\")\n",
        "    print(f\"Test set: {X_test.shape}\")\n",
        "\n",
        "    save_checkpoint({\n",
        "        'X_train': X_train, 'X_test': X_test,\n",
        "        'y_train': y_train, 'y_test': y_test,\n",
        "        'feature_names': list(X.columns)\n",
        "    }, 'train_test_split.pkl')\n",
        "\n",
        "    # Free memory\n",
        "    del train_df\n",
        "    gc.collect()\n",
        "else:\n",
        "    X_train = split_data['X_train']\n",
        "    X_test = split_data['X_test']\n",
        "    y_train = split_data['y_train']\n",
        "    y_test = split_data['y_test']\n",
        "    print(f\"Train set: {X_train.shape}\")\n",
        "    print(f\"Test set: {X_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "al2tIm0Kwry_",
        "outputId": "2dbe28c3-d88a-40cf-beba-608782a498ba"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fiModelDescriptor_freq        1\n",
            "ProductGroup_SSL              0\n",
            "ProductSize_Large             0\n",
            "Enclosure_EROPS w AC          0\n",
            "UnemploymentRate              1\n",
            "ProductSize_Large / Medium    0\n",
            "SalePrice                     0\n",
            "fiSecondaryDesc_freq          1\n",
            "Estimated_Hours_log           4\n",
            "Ripper_Yes                    0\n",
            "ProductSize_Medium            0\n",
            "dtype: int64\n",
            "Total missing values: 7\n",
            "  Filled fiModelDescriptor_freq with median: 329206.0\n",
            "  Filled UnemploymentRate with median: 5.599999904632568\n",
            "  Filled fiSecondaryDesc_freq with median: 39251.0\n",
            "  Filled Estimated_Hours_log with median: 0.0\n",
            "Missing values after handling: 0\n",
            "Checkpoint saved: processed_data_imputed.pkl\n",
            "Checkpoint loaded: train_test_split.pkl\n",
            "Train set: (336996, 10)\n",
            "Test set: (84250, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oekC290Usljb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a48535d-6d15-494e-e5ee-2c0cf5e9efa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed: ProductGroup_SSL\n",
            "Fixed: ProductSize_Large\n",
            "Fixed: ProductSize_Large / Medium\n",
            "Fixed: ProductSize_Medium\n",
            "Fixed: Ripper_Yes\n",
            "Fixed: Enclosure_EROPS w AC\n",
            "Checkpoint saved: train_data_fixed.pkl\n",
            "Data preprocessing complete!\n"
          ]
        }
      ],
      "source": [
        "# --skip--\n",
        "if processed_data is None:\n",
        "    boolean_cols = ['ProductGroup_SSL', 'ProductSize_Large', 'ProductSize_Large / Medium',\n",
        "                    'ProductSize_Medium', 'Ripper_Yes', 'Enclosure_EROPS w AC']\n",
        "    for col in boolean_cols:\n",
        "        if col in train_df.columns:\n",
        "            train_df[col] = train_df[col].map({'True': 1, 'False': 0, True: 1, False: 0})\n",
        "            train_df[col] = train_df[col].astype('int8')\n",
        "            print(f\"Fixed: {col}\")\n",
        "\n",
        "    save_checkpoint(train_df, 'train_data_fixed.pkl')\n",
        "    print(\"Data preprocessing complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08nuK-pYsljb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88afc7a1-81df-42b8-fbe2-b5f5a131462a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint loaded: train_test_split.pkl\n",
            "Train set: (336996, 10)\n",
            "Test set: (84250, 10)\n"
          ]
        }
      ],
      "source": [
        "# --skip--\n",
        "\n",
        "split_data = load_checkpoint('train_test_split.pkl')\n",
        "\n",
        "if split_data is None:\n",
        "    print(\"Creating train-test split...\")\n",
        "\n",
        "    X = train_df.drop('SalePrice', axis=1)\n",
        "    y = train_df['SalePrice']\n",
        "\n",
        "    print(f\"Features shape: {X.shape}\")\n",
        "    print(f\"Target shape: {y.shape}\")\n",
        "    print(f\"Target range: ${y.min():,.0f} - ${y.max():,.0f}\")\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"Train set: {X_train.shape}\")\n",
        "    print(f\"Test set: {X_test.shape}\")\n",
        "\n",
        "    save_checkpoint({\n",
        "        'X_train': X_train, 'X_test': X_test,\n",
        "        'y_train': y_train, 'y_test': y_test,\n",
        "        'feature_names': list(X.columns)\n",
        "    }, 'train_test_split.pkl')\n",
        "\n",
        "    # Free memory\n",
        "    del train_df\n",
        "    gc.collect()\n",
        "else:\n",
        "    X_train = split_data['X_train']\n",
        "    X_test = split_data['X_test']\n",
        "    y_train = split_data['y_train']\n",
        "    y_test = split_data['y_test']\n",
        "    print(f\"Train set: {X_train.shape}\")\n",
        "    print(f\"Test set: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --skip-- Checking for missing values\n",
        "print(train_df.isnull().sum())\n",
        "print(f\"Total missing values: {train_df.isnull().sum().sum()}\")\n",
        "\n",
        "if train_df.isnull().sum().sum() > 0:\n",
        "    numeric_cols = train_df.select_dtypes(include=[np.number]).columns\n",
        "    for col in numeric_cols:\n",
        "        if train_df[col].isnull().sum() > 0:\n",
        "            median_value = train_df[col].median()\n",
        "            train_df[col].fillna(median_value, inplace=True)\n",
        "            print(f\"  Filled {col} with median: {median_value}\")\n",
        "\n",
        "    print(f\"Missing values after handling: {train_df.isnull().sum().sum()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2xySjJRuNpQ",
        "outputId": "7291189a-41a5-4af3-9afb-f11eed326356"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fiModelDescriptor_freq        0\n",
            "ProductGroup_SSL              0\n",
            "ProductSize_Large             0\n",
            "Enclosure_EROPS w AC          0\n",
            "UnemploymentRate              0\n",
            "ProductSize_Large / Medium    0\n",
            "SalePrice                     0\n",
            "fiSecondaryDesc_freq          0\n",
            "Estimated_Hours_log           0\n",
            "Ripper_Yes                    0\n",
            "ProductSize_Medium            0\n",
            "dtype: int64\n",
            "Total missing values: 0\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --run next and continue-- Checking for existing scaler (better safe than sorry)\n",
        "scaler_data = load_checkpoint('scaler_data.pkl')\n",
        "\n",
        "split_data = load_checkpoint('train_test_split.pkl')\n",
        "X_train = split_data['X_train']\n",
        "X_test = split_data['X_test']\n",
        "y_train = split_data['y_train']\n",
        "\n",
        "if scaler_data is None:\n",
        "    print(\"Scaling features...\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    print(f\"Scaled train shape: {X_train_scaled.shape}\")\n",
        "    print(f\"Scaled test shape: {X_test_scaled.shape}\")\n",
        "\n",
        "    save_checkpoint({\n",
        "        'scaler': scaler,\n",
        "        'X_train_scaled': X_train_scaled,\n",
        "        'X_test_scaled': X_test_scaled\n",
        "    }, 'scaler_data.pkl')\n",
        "else:\n",
        "    scaler = scaler_data['scaler']\n",
        "    X_train_scaled = scaler_data['X_train_scaled']\n",
        "    X_test_scaled = scaler_data['X_test_scaled']\n",
        "\n",
        "print(f\"NaN in X_train_scaled: {np.isnan(X_train_scaled).sum()}\")\n",
        "print(f\"NaN in X_test_scaled: {np.isnan(X_test_scaled).sum()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6LAQYfFxH36",
        "outputId": "5e4e951f-4c90-4a96-e019-2dc423c315d4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint loaded: scaler_data.pkl\n",
            "Checkpoint loaded: train_test_split.pkl\n",
            "NaN in X_train_scaled: 0\n",
            "NaN in X_test_scaled: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "iv8G9tiusljb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef846271-397e-4faf-aad4-b6ad2098b6e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint loaded: model_results.pkl\n",
            "Loaded existing results with 10 models\n",
            "Existing models:\n",
            "  - Linear Regression: RMSE=$16,470.23, R²=0.4423\n",
            "  - Decision Tree: RMSE=$12,545.07, R²=0.6764\n",
            "  - Random Forest: RMSE=$12,261.98, R²=0.6909\n",
            "  - Linear Regression: RMSE=$16,470.23, R²=0.4423\n",
            "  - Decision Tree: RMSE=$12,545.07, R²=0.6764\n",
            "  - Random Forest: RMSE=$12,261.98, R²=0.6909\n",
            "  - XGBoost: RMSE=$12,280.46, R²=0.6899\n",
            "  - LightGBM: RMSE=$12,445.48, R²=0.6815\n",
            "  - CatBoost: RMSE=$12,297.45, R²=0.6891\n",
            "  - Neural Network: RMSE=$15,981.33, R²=0.4749\n"
          ]
        }
      ],
      "source": [
        "results_df = load_checkpoint('model_results.pkl')\n",
        "\n",
        "if results_df is None:\n",
        "    results_df = pd.DataFrame(columns=['model', 'rmse', 'mae', 'r2', 'mape', 'timestamp'])\n",
        "    print(\"Created new results tracking DataFrame\")\n",
        "else:\n",
        "    print(f\"Loaded existing results with {len(results_df)} models\")\n",
        "    print(\"Existing models:\")\n",
        "    for idx, row in results_df.iterrows():\n",
        "        print(f\"  - {row['model']}: RMSE=${row['rmse']:,.2f}, R²={row['r2']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = load_checkpoint('model_results.pkl')\n",
        "\n",
        "if results_df is None:\n",
        "    results_df = pd.DataFrame(columns=['model', 'rmse', 'mae', 'r2', 'mape', 'timestamp'])\n",
        "else:\n",
        "    print(f\"Loaded existing results with {len(results_df)} models\")\n",
        "    print(\"Existing models:\")\n",
        "    for idx, row in results_df.iterrows():\n",
        "        print(f\"  - {row['model']}: RMSE=${row['rmse']:,.2f}, R²={row['r2']:.4f}\")"
      ],
      "metadata": {
        "id": "EIITPZpTtIUR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "057c1cf9-012f-4753-dce5-518399695778"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint loaded: model_results.pkl\n",
            "Loaded existing results with 10 models\n",
            "Existing models:\n",
            "  - Linear Regression: RMSE=$16,470.23, R²=0.4423\n",
            "  - Decision Tree: RMSE=$12,545.07, R²=0.6764\n",
            "  - Random Forest: RMSE=$12,261.98, R²=0.6909\n",
            "  - Linear Regression: RMSE=$16,470.23, R²=0.4423\n",
            "  - Decision Tree: RMSE=$12,545.07, R²=0.6764\n",
            "  - Random Forest: RMSE=$12,261.98, R²=0.6909\n",
            "  - XGBoost: RMSE=$12,280.46, R²=0.6899\n",
            "  - LightGBM: RMSE=$12,445.48, R²=0.6815\n",
            "  - CatBoost: RMSE=$12,297.45, R²=0.6891\n",
            "  - Neural Network: RMSE=$15,981.33, R²=0.4749\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline Models"
      ],
      "metadata": {
        "id": "CNVLdAxkzDOr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression"
      ],
      "metadata": {
        "id": "5t7y_1lKtXFR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sl2TEOoEtDsl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1639c428-b504-4d76-ce33-1bf7da273a11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint loaded: linear_regression.pkl\n",
            "Loaded Linear Regression from checkpoint\n"
          ]
        }
      ],
      "source": [
        "lr_checkpoint = load_checkpoint('linear_regression.pkl')\n",
        "\n",
        "if lr_checkpoint is None:\n",
        "    print(\"Training Linear Regression...\")\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "\n",
        "    lr_model = LinearRegression()\n",
        "    lr_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    save_checkpoint({'model': lr_model}, 'linear_regression.pkl')\n",
        "else:\n",
        "    lr_model = lr_checkpoint['model']\n",
        "    print(\"Loaded Linear Regression from checkpoint\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAednhoutDsl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bb3feb4-440c-42b8-9661-d1f6b7a135bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions shape: (84250,)\n",
            "Prediction range: $762 - $92,143\n"
          ]
        }
      ],
      "source": [
        "lr_pred = lr_model.predict(X_test_scaled)\n",
        "print(f\"Predictions shape: {lr_pred.shape}\")\n",
        "print(f\"Prediction range: ${lr_pred.min():,.0f} - ${lr_pred.max():,.0f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for NaN values in scaled data\n",
        "print(\"Checking for NaN values in scaled data...\")\n",
        "print(f\"NaN in X_train_scaled: {np.isnan(X_train_scaled).sum()}\")\n",
        "print(f\"NaN in X_test_scaled: {np.isnan(X_test_scaled).sum()}\")\n",
        "\n",
        "# If there are NaN values, we need to re-scale the data\n",
        "if np.isnan(X_train_scaled).sum() > 0 or np.isnan(X_test_scaled).sum() > 0:\n",
        "    print(\"\\nRe-scaling data after handling missing values...\")\n",
        "\n",
        "    # Delete the old scaler checkpoint\n",
        "    import os\n",
        "    scaler_path = os.path.join(checkpoint_dir, 'scaler_data.pkl')\n",
        "    if os.path.exists(scaler_path):\n",
        "        os.remove(scaler_path)\n",
        "        print(\"Removed old scaler checkpoint\")\n",
        "\n",
        "    # Reload the cleaned data\n",
        "    split_data = load_checkpoint('train_test_split.pkl')\n",
        "    X_train = split_data['X_train']\n",
        "    X_test = split_data['X_test']\n",
        "\n",
        "    # Check for any remaining NaN values\n",
        "    print(f\"\\nNaN in X_train before scaling: {X_train.isnull().sum().sum()}\")\n",
        "    print(f\"NaN in X_test before scaling: {X_test.isnull().sum().sum()}\")\n",
        "\n",
        "    # Create new scaler and scale the data\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Verify no NaN values after scaling\n",
        "    print(f\"\\nAfter re-scaling:\")\n",
        "    print(f\"NaN in X_train_scaled: {np.isnan(X_train_scaled).sum()}\")\n",
        "    print(f\"NaN in X_test_scaled: {np.isnan(X_test_scaled).sum()}\")\n",
        "\n",
        "    # Save the new scaler and scaled data\n",
        "    save_checkpoint({\n",
        "        'scaler': scaler,\n",
        "        'X_train_scaled': X_train_scaled,\n",
        "        'X_test_scaled': X_test_scaled\n",
        "    }, 'scaler_data.pkl')\n",
        "\n",
        "    print(\"\\nScaling completed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNP98emRvtjW",
        "outputId": "f5a989c6-d484-49d5-fda0-d25282910588"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking for NaN values in scaled data...\n",
            "NaN in X_train_scaled: 0\n",
            "NaN in X_test_scaled: 0\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MP_i-NcUtDsl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f77c808-a510-46c1-d2bb-54fa1697374b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Model: Linear Regression\n",
            "==================================================\n",
            "RMSE: $16,470.23\n",
            "MAE:  $11,863.15\n",
            "R²:   0.4423\n",
            "MAPE: 49.75%\n",
            "==================================================\n",
            "Checkpoint saved: model_results.pkl\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "lr_metrics = calculate_metrics(y_test, lr_pred, 'Linear Regression')\n",
        "print_metrics(lr_metrics)\n",
        "\n",
        "results_df = pd.concat([results_df, pd.DataFrame([{**lr_metrics, 'timestamp': datetime.now()}])], ignore_index=True)\n",
        "save_checkpoint(results_df, 'model_results.pkl')\n",
        "\n",
        "del lr_model, lr_pred\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree"
      ],
      "metadata": {
        "id": "UhmrMXoFyDY6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqUIv7DNtDsl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f476292-443b-481e-fca7-0691bce2575b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint loaded: decision_tree.pkl\n",
            "Loaded Decision Tree from checkpoint\n",
            "Best parameters: {'max_depth': 20, 'min_samples_leaf': 10, 'min_samples_split': 10}\n"
          ]
        }
      ],
      "source": [
        "dt_checkpoint = load_checkpoint('decision_tree.pkl')\n",
        "\n",
        "if dt_checkpoint is None:\n",
        "    print(\"Training Decision Tree with hyperparameter tuning...\")\n",
        "    from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "    param_grid = {\n",
        "        'max_depth': [10, 20, 30],\n",
        "        'min_samples_split': [10, 20, 50],\n",
        "        'min_samples_leaf': [5, 10, 20]\n",
        "    }\n",
        "\n",
        "    dt_model = DecisionTreeRegressor(random_state=42)\n",
        "    dt_grid = GridSearchCV(dt_model, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "\n",
        "    print(\"Running grid search...\")\n",
        "    dt_grid.fit(X_train, y_train)\n",
        "\n",
        "    print(f\"Best parameters: {dt_grid.best_params_}\")\n",
        "\n",
        "    save_checkpoint({\n",
        "        'model': dt_grid.best_estimator_,\n",
        "        'best_params': dt_grid.best_params_,\n",
        "        'cv_results': dt_grid.cv_results_\n",
        "    }, 'decision_tree.pkl')\n",
        "\n",
        "    dt_model = dt_grid.best_estimator_\n",
        "else:\n",
        "    dt_model = dt_checkpoint['model']\n",
        "    print(f\"Loaded Decision Tree from checkpoint\")\n",
        "    print(f\"Best parameters: {dt_checkpoint['best_params']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERJ_J4QEtDsl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83460f9f-62fa-4157-e592-3f85af63b426"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(84250,)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "dt_pred = dt_model.predict(X_test)\n",
        "dt_pred.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lM_a7crtDsm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0de60439-118b-492f-bfe1-9d28443008f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Model: Decision Tree\n",
            "==================================================\n",
            "RMSE: $12,545.07\n",
            "MAE:  $8,508.08\n",
            "R²:   0.6764\n",
            "MAPE: 33.88%\n",
            "==================================================\n",
            "Checkpoint saved: model_results.pkl\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "dt_metrics = calculate_metrics(y_test, dt_pred, 'Decision Tree')\n",
        "print_metrics(dt_metrics)\n",
        "\n",
        "results_df = pd.concat([results_df, pd.DataFrame([{**dt_metrics, 'timestamp': datetime.now()}])], ignore_index=True)\n",
        "save_checkpoint(results_df, 'model_results.pkl')\n",
        "\n",
        "del dt_model, dt_pred\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {
        "id": "ST1zSV9YyhHd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGyC8P4RtDsm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e9fa5dc-babd-4f91-affd-99edb5b6cafe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint loaded: random_forest.pkl\n",
            "Loaded Random Forest from checkpoint\n"
          ]
        }
      ],
      "source": [
        "rf_checkpoint = load_checkpoint('random_forest.pkl')\n",
        "\n",
        "if rf_checkpoint is None:\n",
        "    print(\"Training Random Forest...\")\n",
        "    from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "    rf_model = RandomForestRegressor(\n",
        "        n_estimators=100,\n",
        "        max_depth=20,\n",
        "        min_samples_split=10,\n",
        "        min_samples_leaf=5,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    rf_model.fit(X_train, y_train)\n",
        "\n",
        "    save_checkpoint({'model': rf_model}, 'random_forest.pkl')\n",
        "else:\n",
        "    rf_model = rf_checkpoint['model']\n",
        "    print(\"Loaded Random Forest from checkpoint\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJPxDriQtDsm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eab9db0c-0cb6-4460-fba5-f847c7a257c0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(84250,)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "rf_pred = rf_model.predict(X_test)\n",
        "rf_pred.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtV4rXpCtDsm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27c76b00-b268-412e-dbac-a818879dc30c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Model: Random Forest\n",
            "==================================================\n",
            "RMSE: $12,261.98\n",
            "MAE:  $8,322.52\n",
            "R²:   0.6909\n",
            "MAPE: 33.31%\n",
            "==================================================\n",
            "Top 5 Important Features:\n",
            "                  feature  importance\n",
            "3    Enclosure_EROPS w AC    0.275235\n",
            "6    fiSecondaryDesc_freq    0.238333\n",
            "4        UnemploymentRate    0.093456\n",
            "7     Estimated_Hours_log    0.083271\n",
            "0  fiModelDescriptor_freq    0.078357\n",
            "Checkpoint saved: model_results.pkl\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "46"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "rf_metrics = calculate_metrics(y_test, rf_pred, 'Random Forest')\n",
        "print_metrics(rf_metrics)\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X_train.columns,\n",
        "    'importance': rf_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"Top 5 Important Features:\")\n",
        "print(feature_importance.head())\n",
        "\n",
        "results_df = pd.concat([results_df, pd.DataFrame([{**rf_metrics, 'timestamp': datetime.now()}])], ignore_index=True)\n",
        "save_checkpoint(results_df, 'model_results.pkl')\n",
        "\n",
        "del rf_model, rf_pred\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Models"
      ],
      "metadata": {
        "id": "tm8yAki4zM6p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost"
      ],
      "metadata": {
        "id": "foAhbWaVzRFP"
      }
    },
    {
      "source": [
        "# model updated\n",
        "xgb_checkpoint = load_checkpoint('xgboost.pkl')\n",
        "\n",
        "if xgb_checkpoint is None:\n",
        "    print(\"Training XGBoost with GPU...\")\n",
        "    import xgboost as xgb\n",
        "    import torch\n",
        "    gpu_available = torch.cuda.is_available()\n",
        "\n",
        "    xgb_params = {\n",
        "        'tree_method': 'gpu_hist' if gpu_available else 'hist',\n",
        "        'gpu_id': 0 if gpu_available else None,\n",
        "        'max_depth': 10,\n",
        "        'learning_rate': 0.1,\n",
        "        'n_estimators': 100,\n",
        "        'objective': 'reg:squarederror',\n",
        "        'random_state': 42,\n",
        "    }\n",
        "\n",
        "    xgb_params = {k: v for k, v in xgb_params.items() if v is not None}\n",
        "    print(f\"Using {'GPU' if gpu_available else 'CPU'} for training\")\n",
        "\n",
        "    xgb_model = xgb.XGBRegressor(**xgb_params)\n",
        "\n",
        "    xgb_model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_test, y_test)],\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    best_iter = None\n",
        "    if hasattr(xgb_model, 'best_iteration'):\n",
        "        best_iter = xgb_model.best_iteration\n",
        "        print(f\"Best iteration: {best_iter}\")\n",
        "    else:\n",
        "        print(f\"Trained for {xgb_model.n_estimators} iterations (no early stopping callbacks specified)\")\n",
        "\n",
        "\n",
        "    save_checkpoint({\n",
        "        'model': xgb_model,\n",
        "        'params': xgb_params,\n",
        "        'best_iteration': best_iter # This might be None\n",
        "    }, 'xgboost.pkl')\n",
        "else:\n",
        "    import xgboost as xgb\n",
        "    xgb_model = xgb_checkpoint['model']\n",
        "    print(f\"Loaded XGBoost from checkpoint (best iteration: {xgb_checkpoint.get('best_iteration', 'N/A')})\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_BS7Dv-1BXN",
        "outputId": "41f84f9a-2f8e-466e-86cb-129db36610a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training XGBoost with GPU...\n",
            "Using GPU for training\n",
            "[0]\tvalidation_0-rmse:20701.01457\n",
            "[1]\tvalidation_0-rmse:19534.11546\n",
            "[2]\tvalidation_0-rmse:18540.69137\n",
            "[3]\tvalidation_0-rmse:17694.98160\n",
            "[4]\tvalidation_0-rmse:16971.34938\n",
            "[5]\tvalidation_0-rmse:16358.00403\n",
            "[6]\tvalidation_0-rmse:15824.02202\n",
            "[7]\tvalidation_0-rmse:15379.43633\n",
            "[8]\tvalidation_0-rmse:14993.61039\n",
            "[9]\tvalidation_0-rmse:14671.32443\n",
            "[10]\tvalidation_0-rmse:14396.05122\n",
            "[11]\tvalidation_0-rmse:14162.49557\n",
            "[12]\tvalidation_0-rmse:13960.50707\n",
            "[13]\tvalidation_0-rmse:13800.10765\n",
            "[14]\tvalidation_0-rmse:13652.53331\n",
            "[15]\tvalidation_0-rmse:13525.43155\n",
            "[16]\tvalidation_0-rmse:13408.51841\n",
            "[17]\tvalidation_0-rmse:13311.12589\n",
            "[18]\tvalidation_0-rmse:13234.91860\n",
            "[19]\tvalidation_0-rmse:13160.26888\n",
            "[20]\tvalidation_0-rmse:13090.10332\n",
            "[21]\tvalidation_0-rmse:13035.90565\n",
            "[22]\tvalidation_0-rmse:12983.47115\n",
            "[23]\tvalidation_0-rmse:12944.85862\n",
            "[24]\tvalidation_0-rmse:12900.16835\n",
            "[25]\tvalidation_0-rmse:12870.70169\n",
            "[26]\tvalidation_0-rmse:12843.95213\n",
            "[27]\tvalidation_0-rmse:12812.11542\n",
            "[28]\tvalidation_0-rmse:12779.32430\n",
            "[29]\tvalidation_0-rmse:12752.20308\n",
            "[30]\tvalidation_0-rmse:12729.03597\n",
            "[31]\tvalidation_0-rmse:12705.73217\n",
            "[32]\tvalidation_0-rmse:12685.78674\n",
            "[33]\tvalidation_0-rmse:12672.37466\n",
            "[34]\tvalidation_0-rmse:12649.17383\n",
            "[35]\tvalidation_0-rmse:12632.82657\n",
            "[36]\tvalidation_0-rmse:12620.22701\n",
            "[37]\tvalidation_0-rmse:12605.80921\n",
            "[38]\tvalidation_0-rmse:12591.68436\n",
            "[39]\tvalidation_0-rmse:12576.22581\n",
            "[40]\tvalidation_0-rmse:12560.60984\n",
            "[41]\tvalidation_0-rmse:12548.14445\n",
            "[42]\tvalidation_0-rmse:12538.44653\n",
            "[43]\tvalidation_0-rmse:12528.65941\n",
            "[44]\tvalidation_0-rmse:12519.96219\n",
            "[45]\tvalidation_0-rmse:12508.07674\n",
            "[46]\tvalidation_0-rmse:12496.67407\n",
            "[47]\tvalidation_0-rmse:12486.89647\n",
            "[48]\tvalidation_0-rmse:12475.34424\n",
            "[49]\tvalidation_0-rmse:12464.90951\n",
            "[50]\tvalidation_0-rmse:12456.18751\n",
            "[51]\tvalidation_0-rmse:12449.66852\n",
            "[52]\tvalidation_0-rmse:12438.92036\n",
            "[53]\tvalidation_0-rmse:12431.13176\n",
            "[54]\tvalidation_0-rmse:12425.51935\n",
            "[55]\tvalidation_0-rmse:12418.51240\n",
            "[56]\tvalidation_0-rmse:12410.72544\n",
            "[57]\tvalidation_0-rmse:12404.52598\n",
            "[58]\tvalidation_0-rmse:12399.54556\n",
            "[59]\tvalidation_0-rmse:12392.85418\n",
            "[60]\tvalidation_0-rmse:12389.25700\n",
            "[61]\tvalidation_0-rmse:12384.53069\n",
            "[62]\tvalidation_0-rmse:12379.42096\n",
            "[63]\tvalidation_0-rmse:12375.70519\n",
            "[64]\tvalidation_0-rmse:12371.55897\n",
            "[65]\tvalidation_0-rmse:12368.25026\n",
            "[66]\tvalidation_0-rmse:12364.60901\n",
            "[67]\tvalidation_0-rmse:12362.22996\n",
            "[68]\tvalidation_0-rmse:12357.43344\n",
            "[69]\tvalidation_0-rmse:12354.71237\n",
            "[70]\tvalidation_0-rmse:12351.14983\n",
            "[71]\tvalidation_0-rmse:12349.09724\n",
            "[72]\tvalidation_0-rmse:12345.19426\n",
            "[73]\tvalidation_0-rmse:12340.31602\n",
            "[74]\tvalidation_0-rmse:12336.21054\n",
            "[75]\tvalidation_0-rmse:12334.30489\n",
            "[76]\tvalidation_0-rmse:12331.15442\n",
            "[77]\tvalidation_0-rmse:12327.71590\n",
            "[78]\tvalidation_0-rmse:12324.71771\n",
            "[79]\tvalidation_0-rmse:12321.90675\n",
            "[80]\tvalidation_0-rmse:12320.09838\n",
            "[81]\tvalidation_0-rmse:12317.26592\n",
            "[82]\tvalidation_0-rmse:12315.75291\n",
            "[83]\tvalidation_0-rmse:12312.67788\n",
            "[84]\tvalidation_0-rmse:12309.36159\n",
            "[85]\tvalidation_0-rmse:12308.25087\n",
            "[86]\tvalidation_0-rmse:12305.29165\n",
            "[87]\tvalidation_0-rmse:12300.43311\n",
            "[88]\tvalidation_0-rmse:12298.69571\n",
            "[89]\tvalidation_0-rmse:12297.35148\n",
            "[90]\tvalidation_0-rmse:12296.73663\n",
            "[91]\tvalidation_0-rmse:12294.67091\n",
            "[92]\tvalidation_0-rmse:12291.93117\n",
            "[93]\tvalidation_0-rmse:12288.89134\n",
            "[94]\tvalidation_0-rmse:12288.04469\n",
            "[95]\tvalidation_0-rmse:12287.68275\n",
            "[96]\tvalidation_0-rmse:12285.07572\n",
            "[97]\tvalidation_0-rmse:12284.08579\n",
            "[98]\tvalidation_0-rmse:12282.02517\n",
            "[99]\tvalidation_0-rmse:12280.46270\n",
            "Trained for 100 iterations (no early stopping callbacks specified)\n",
            "Checkpoint saved: xgboost.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYKawj_8tDsm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52ea4dd8-a56d-42c5-e39a-e75f80c6b74c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions shape: (84250,)\n"
          ]
        }
      ],
      "source": [
        "xgb_pred = xgb_model.predict(X_test)\n",
        "print(f\"Predictions shape: {xgb_pred.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kic_fF4otDsm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdeb5bda-205f-4119-d033-d4b063b19d37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Model: XGBoost\n",
            "==================================================\n",
            "RMSE: $12,280.46\n",
            "MAE:  $8,413.97\n",
            "R²:   0.6899\n",
            "MAPE: 33.97%\n",
            "==================================================\n",
            "Checkpoint saved: model_results.pkl\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "85"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "xgb_metrics = calculate_metrics(y_test, xgb_pred, 'XGBoost')\n",
        "print_metrics(xgb_metrics)\n",
        "\n",
        "results_df = pd.concat([results_df, pd.DataFrame([{**xgb_metrics, 'timestamp': datetime.now()}])], ignore_index=True)\n",
        "save_checkpoint(results_df, 'model_results.pkl')\n",
        "\n",
        "del xgb_model, xgb_pred\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LightGBM"
      ],
      "metadata": {
        "id": "db00YdCn1iCH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jQdn-EDtDsm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f77d8df6-2869-4b69-c71d-02f044543c52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training LightGBM...\n",
            "Training until validation scores don't improve for 10 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1000]\tvalid_0's rmse: 12445.5\n",
            "Best iteration: 1000\n",
            "Checkpoint saved: lightgbm.pkl\n"
          ]
        }
      ],
      "source": [
        "import lightgbm as lgb\n",
        "\n",
        "lgb_checkpoint = load_checkpoint('lightgbm.pkl')\n",
        "\n",
        "if lgb_checkpoint is None:\n",
        "    print(\"Training LightGBM...\")\n",
        "    lgb_params = {\n",
        "        'device': 'cpu',  # Forceing CPU mode for Colab compatibility\n",
        "        'objective': 'regression',\n",
        "        'metric': 'rmse',\n",
        "        'num_leaves': 31,\n",
        "        'learning_rate': 0.05,\n",
        "        'feature_fraction': 0.9,\n",
        "        'bagging_fraction': 0.8,\n",
        "        'bagging_freq': 5,\n",
        "        'verbose': -1,\n",
        "        'random_state': 42,\n",
        "        'num_threads': -1  # Using all available CPU threads\n",
        "    }\n",
        "    lgb_train = lgb.Dataset(X_train, y_train)\n",
        "    lgb_valid = lgb.Dataset(X_test, y_test)\n",
        "\n",
        "    lgb_model = lgb.train(\n",
        "        lgb_params,\n",
        "        lgb_train,\n",
        "        valid_sets=[lgb_valid],\n",
        "        num_boost_round=1000,\n",
        "        callbacks=[lgb.early_stopping(10), lgb.log_evaluation(0)]\n",
        "    )\n",
        "\n",
        "    print(f\"Best iteration: {lgb_model.best_iteration}\")\n",
        "\n",
        "    save_checkpoint({\n",
        "        'model': lgb_model,\n",
        "        'params': lgb_params,\n",
        "        'best_iteration': lgb_model.best_iteration\n",
        "    }, 'lightgbm.pkl')\n",
        "else:\n",
        "    import lightgbm as lgb\n",
        "    lgb_model = lgb_checkpoint['model']\n",
        "    print(f\"Loaded LightGBM from checkpoint (best iteration: {lgb_checkpoint.get('best_iteration', 'N/A')})\")"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzCRAMK0tDsm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "318f272f-7ed0-4154-92e2-491d8192fe65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions shape: (84250,)\n"
          ]
        }
      ],
      "source": [
        "lgb_pred = lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration)\n",
        "print(f\"Predictions shape: {lgb_pred.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxgAAZ3RtDsm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26deb538-bd61-4af5-f34c-38aa2b504295"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Model: LightGBM\n",
            "==================================================\n",
            "RMSE: $12,445.48\n",
            "MAE:  $8,601.73\n",
            "R²:   0.6815\n",
            "MAPE: 34.89%\n",
            "==================================================\n",
            "Checkpoint saved: model_results.pkl\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3087"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "lgb_metrics = calculate_metrics(y_test, lgb_pred, 'LightGBM')\n",
        "print_metrics(lgb_metrics)\n",
        "\n",
        "results_df = pd.concat([results_df, pd.DataFrame([{**lgb_metrics, 'timestamp': datetime.now()}])], ignore_index=True)\n",
        "save_checkpoint(results_df, 'model_results.pkl')\n",
        "\n",
        "del lgb_model, lgb_pred\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CatBoost"
      ],
      "metadata": {
        "id": "eKg4lnjw38kl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5k4zyiitDsn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d73f9d65-fdfd-4fef-d329-5223885387be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.15.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.1.2)\n",
            "Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n",
            "Training CatBoost with GPU...\n",
            "Best iteration: 999\n",
            "Checkpoint saved: catboost.pkl\n"
          ]
        }
      ],
      "source": [
        "cb_checkpoint = load_checkpoint('catboost.pkl')\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "if cb_checkpoint is None:\n",
        "    print(\"Training CatBoost with GPU...\")\n",
        "    gpu_available = torch.cuda.is_available()\n",
        "\n",
        "    cb_params = {\n",
        "        'iterations': 1000,\n",
        "        'learning_rate': 0.1,\n",
        "        'depth': 10,\n",
        "        'loss_function': 'RMSE',\n",
        "        'random_state': 42,\n",
        "        'verbose': False\n",
        "    }\n",
        "    if gpu_available:\n",
        "        cb_params.update({\n",
        "            'task_type': 'GPU',\n",
        "            'devices': '0'\n",
        "        })\n",
        "    cb_model = CatBoostRegressor(**cb_params)\n",
        "    cb_model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=(X_test, y_test),\n",
        "        early_stopping_rounds=10,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    print(f\"Best iteration: {cb_model.best_iteration_}\")\n",
        "\n",
        "    save_checkpoint({\n",
        "        'model': cb_model,\n",
        "        'params': cb_params,\n",
        "        'best_iteration': cb_model.best_iteration_\n",
        "    }, 'catboost.pkl')\n",
        "else:\n",
        "    from catboost import CatBoostRegressor\n",
        "    cb_model = cb_checkpoint['model']\n",
        "    print(f\"Loaded CatBoost from checkpoint (best iteration: {cb_checkpoint.get('best_iteration', 'N/A')})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLuVy6KNtDsq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dad0ce72-62e7-44b0-bf10-0dffc58714ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions shape: (84250,)\n"
          ]
        }
      ],
      "source": [
        "cb_pred = cb_model.predict(X_test)\n",
        "print(f\"Predictions shape: {cb_pred.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGRdU67itDsq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c57298ea-3e04-427d-a254-f933d97a44ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Model: CatBoost\n",
            "==================================================\n",
            "RMSE: $12,297.45\n",
            "MAE:  $8,462.33\n",
            "R²:   0.6891\n",
            "MAPE: 34.28%\n",
            "==================================================\n",
            "Checkpoint saved: model_results.pkl\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "cb_metrics = calculate_metrics(y_test, cb_pred, 'CatBoost')\n",
        "print_metrics(cb_metrics)\n",
        "\n",
        "results_df = pd.concat([results_df, pd.DataFrame([{**cb_metrics, 'timestamp': datetime.now()}])], ignore_index=True)\n",
        "save_checkpoint(results_df, 'model_results.pkl')\n",
        "\n",
        "del cb_model, cb_pred\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network"
      ],
      "metadata": {
        "id": "3t-_wLxV4gez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "metadata": {
        "id": "-sZlKmgc4pHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fu5ui9VtDsq"
      },
      "outputs": [],
      "source": [
        "class DemandNN(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(DemandNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 32)\n",
        "        self.fc4 = nn.Linear(32, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWi0GQG8tDsq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c4401e5-43a1-46d9-ba3c-96208bb944d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Neural Network on GPU...\n",
            "Using device: cuda\n",
            "Training...\n",
            "Epoch [10/100], Train Loss: 268905699.3652, Val Loss: 259120384.0000\n",
            "Epoch [20/100], Train Loss: 267352000.1579, Val Loss: 257026816.0000\n",
            "Epoch [30/100], Train Loss: 267111200.0972, Val Loss: 256508528.0000\n",
            "Epoch [40/100], Train Loss: 266357220.5194, Val Loss: 256134400.0000\n",
            "Epoch [50/100], Train Loss: 266187569.4822, Val Loss: 255855728.0000\n",
            "Epoch [60/100], Train Loss: 265714215.6659, Val Loss: 255923040.0000\n",
            "Epoch [70/100], Train Loss: 265652392.0182, Val Loss: 255483296.0000\n",
            "Epoch [80/100], Train Loss: 265815041.3242, Val Loss: 255463312.0000\n",
            "Epoch [90/100], Train Loss: 265831086.5057, Val Loss: 255444912.0000\n",
            "Early stopping at epoch 96\n",
            "Checkpoint saved: neural_network.pkl\n",
            "Training complete. Best validation loss: 255402736.0000\n"
          ]
        }
      ],
      "source": [
        "nn_checkpoint = load_checkpoint('neural_network.pkl')\n",
        "\n",
        "if nn_checkpoint is None:\n",
        "    print(\"Training Neural Network on GPU...\")\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "    X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
        "    y_train_tensor = torch.FloatTensor(y_train.values).reshape(-1, 1)\n",
        "    X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
        "    y_test_tensor = torch.FloatTensor(y_test.values).reshape(-1, 1)\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
        "\n",
        "    model = DemandNN(X_train_scaled.shape[1]).to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
        "    best_loss = float('inf')\n",
        "    patience = 10\n",
        "    counter = 0\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    print(\"Training...\")\n",
        "    for epoch in range(100):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        avg_train_loss = epoch_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            X_test_device = X_test_tensor.to(device)\n",
        "            y_test_device = y_test_tensor.to(device)\n",
        "            val_outputs = model(X_test_device)\n",
        "            val_loss = criterion(val_outputs, y_test_device).item()\n",
        "            val_losses.append(val_loss)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            counter = 0\n",
        "            torch.save(model.state_dict(), os.path.join(checkpoint_dir, 'nn_best_model.pth'))\n",
        "        else:\n",
        "            counter += 1\n",
        "            if counter >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/100], Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    model.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'nn_best_model.pth')))\n",
        "    save_checkpoint({\n",
        "        'model_state': model.state_dict(),\n",
        "        'input_size': X_train_scaled.shape[1],\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'best_epoch': len(val_losses) - patience\n",
        "    }, 'neural_network.pkl')\n",
        "\n",
        "    print(f\"Training complete. Best validation loss: {best_loss:.4f}\")\n",
        "else:\n",
        "    print(\"Loading Neural Network from checkpoint...\")\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = DemandNN(nn_checkpoint['input_size']).to(device)\n",
        "    model.load_state_dict(nn_checkpoint['model_state'])\n",
        "    model.eval()\n",
        "    print(f\"Loaded model from epoch {nn_checkpoint.get('best_epoch', 'N/A')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Smqe-5MWtDsq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f064b0c9-2be0-46f0-b21a-2669defea6c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions shape: (84250,)\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
        "    nn_pred = model(X_test_tensor).cpu().numpy().flatten()\n",
        "\n",
        "print(f\"Predictions shape: {nn_pred.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7-uOrBPtDsq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "231e3802-6ab4-4531-f4b7-0997873bcfb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Model: Neural Network\n",
            "==================================================\n",
            "RMSE: $15,981.33\n",
            "MAE:  $11,338.11\n",
            "R²:   0.4749\n",
            "MAPE: 46.75%\n",
            "==================================================\n",
            "Checkpoint saved: model_results.pkl\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "nn_metrics = calculate_metrics(y_test, nn_pred, 'Neural Network')\n",
        "print_metrics(nn_metrics)\n",
        "\n",
        "results_df = pd.concat([results_df, pd.DataFrame([{**nn_metrics, 'timestamp': datetime.now()}])], ignore_index=True)\n",
        "save_checkpoint(results_df, 'model_results.pkl')\n",
        "\n",
        "del model, nn_pred\n",
        "if 'X_test_tensor' in locals():\n",
        "    del X_test_tensor\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sWt2LGhtDsq"
      },
      "source": [
        "# Model Comparison Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "WRMUOJ87tDsr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16721332-ed5c-4870-be76-04de770f6fb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint loaded: model_results.pkl\n",
            "\n",
            "================================================================================\n",
            "FINAL MODEL COMPARISON\n",
            "================================================================================\n",
            "\n",
            "Model                        RMSE          MAE       R²     MAPE\n",
            "----------------------------------------------------------------------\n",
            "Random Forest        $  12,261.98 $   8,322.52   0.6909   33.31%\n",
            "XGBoost              $  12,280.46 $   8,413.97   0.6899   33.97%\n",
            "CatBoost             $  12,297.45 $   8,462.33   0.6891   34.28%\n",
            "LightGBM             $  12,445.48 $   8,601.73   0.6815   34.89%\n",
            "Decision Tree        $  12,545.07 $   8,508.08   0.6764   33.88%\n",
            "Neural Network       $  15,981.33 $  11,338.11   0.4749   46.75%\n",
            "Linear Regression    $  16,470.23 $  11,863.15   0.4423   49.75%\n",
            "\n",
            "================================================================================\n",
            "BEST MODEL: Random Forest\n",
            "RMSE: $12,261.98\n",
            "================================================================================\n",
            "Checkpoint saved: best_model_info.pkl\n"
          ]
        }
      ],
      "source": [
        "results_df = load_checkpoint('model_results.pkl')\n",
        "results_df = results_df.sort_values('timestamp').drop_duplicates(subset=['model'], keep='last')\n",
        "\n",
        "# Sort by RMSE (best to worst)\n",
        "results_df = results_df.sort_values('rmse')\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL MODEL COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\n{'Model':<20} {'RMSE':>12} {'MAE':>12} {'R²':>8} {'MAPE':>8}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "for _, row in results_df.iterrows():\n",
        "    print(f\"{row['model']:<20} ${row['rmse']:>11,.2f} ${row['mae']:>11,.2f} {row['r2']:>8.4f} {row['mape']:>7.2f}%\")\n",
        "\n",
        "best_model_idx = results_df['rmse'].idxmin()\n",
        "best_model_name = results_df.loc[best_model_idx, 'model']\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"BEST MODEL: {best_model_name}\")\n",
        "print(f\"RMSE: ${results_df.loc[best_model_idx, 'rmse']:,.2f}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "save_checkpoint({\n",
        "    'best_model': best_model_name,\n",
        "    'best_metrics': results_df.loc[best_model_idx].to_dict()\n",
        "}, 'best_model_info.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "YjVv_FFdtDsr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "252c7ec4-ef1f-4df3-fa64-6be84270fca6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading validation data...\n",
            "Validation data shape: (11573, 11)\n",
            "\n",
            "Columns: ['SalesID', 'fiModelDescriptor_freq', 'ProductSize_Large / Medium', 'ProductGroup_SSL', 'Ripper_Yes', 'ProductSize_Medium', 'ProductSize_Large', 'Enclosure_EROPS w AC', 'UnemploymentRate', 'fiSecondaryDesc_freq', 'Estimated_Hours_log']\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading validation data...\")\n",
        "\n",
        "valid_df = pd.read_csv('/content/drive/MyDrive/data/valid_df_MR_lite.csv')\n",
        "\n",
        "print(f\"Validation data shape: {valid_df.shape}\")\n",
        "print(f\"\\nColumns: {list(valid_df.columns)}\")\n",
        "\n",
        "sales_ids = valid_df['SalesID']\n",
        "X_valid = valid_df.drop('SalesID', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_valid.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vShcQegL751i",
        "outputId": "0a794070-88b5-4425-de6f-4950a23ac239"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fiModelDescriptor_freq          int64\n",
            "ProductSize_Large / Medium      int64\n",
            "ProductGroup_SSL                int64\n",
            "Ripper_Yes                      int64\n",
            "ProductSize_Medium              int64\n",
            "ProductSize_Large               int64\n",
            "Enclosure_EROPS w AC            int64\n",
            "UnemploymentRate              float64\n",
            "fiSecondaryDesc_freq            int64\n",
            "Estimated_Hours_log           float64\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reordering...\n",
        "X_valid = X_valid[feature_names]\n",
        "\n",
        "print(\"Validation features after reordering:\")\n",
        "print(list(X_valid.columns))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8qRZsZF8wPX",
        "outputId": "d3b80a46-fc02-4ed2-b61b-c2e4a2c9e498"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation features after reordering:\n",
            "['fiModelDescriptor_freq', 'ProductGroup_SSL', 'ProductSize_Large', 'Enclosure_EROPS w AC', 'UnemploymentRate', 'ProductSize_Large / Medium', 'fiSecondaryDesc_freq', 'Estimated_Hours_log', 'Ripper_Yes', 'ProductSize_Medium']\n",
            "Column order fixed!\n"
          ]
        }
      ],
      "execution_count": 24
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F72ZPBbMtDsr"
      },
      "source": [
        "## Predictions with Best Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "6N6mVoCGtDsr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a475ad37-80a4-43d0-e690-21a706d8f4cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint loaded: best_model_info.pkl\n",
            "Generating predictions with Random Forest...\n",
            "Checkpoint loaded: random_forest.pkl\n",
            "\n",
            "Predictions generated: 11573 samples\n",
            "Prediction range: $7,599 - $100,969\n",
            "Average prediction: $38,909\n"
          ]
        }
      ],
      "source": [
        "best_model_info = load_checkpoint('best_model_info.pkl')\n",
        "best_model_name = best_model_info['best_model']\n",
        "\n",
        "print(f\"Generating predictions with {best_model_name}...\")\n",
        "\n",
        "if best_model_name == 'Linear Regression':\n",
        "    model_data = load_checkpoint('linear_regression.pkl')\n",
        "    model = model_data['model']\n",
        "    X_valid_scaled = scaler.transform(X_valid)\n",
        "    predictions = model.predict(X_valid_scaled)\n",
        "\n",
        "elif best_model_name == 'Decision Tree':\n",
        "    model_data = load_checkpoint('decision_tree.pkl')\n",
        "    model = model_data['model']\n",
        "    predictions = model.predict(X_valid)\n",
        "\n",
        "elif best_model_name == 'Random Forest':\n",
        "    model_data = load_checkpoint('random_forest.pkl')\n",
        "    model = model_data['model']\n",
        "    predictions = model.predict(X_valid)\n",
        "\n",
        "elif best_model_name == 'XGBoost':\n",
        "    import xgboost as xgb\n",
        "    model_data = load_checkpoint('xgboost.pkl')\n",
        "    model = model_data['model']\n",
        "    predictions = model.predict(X_valid)\n",
        "\n",
        "elif best_model_name == 'LightGBM':\n",
        "    import lightgbm as lgb\n",
        "    model_data = load_checkpoint('lightgbm.pkl')\n",
        "    model = model_data['model']\n",
        "    predictions = model.predict(X_valid, num_iteration=model.best_iteration)\n",
        "\n",
        "elif best_model_name == 'CatBoost':\n",
        "    from catboost import CatBoostRegressor\n",
        "    model_data = load_checkpoint('catboost.pkl')\n",
        "    model = model_data['model']\n",
        "    predictions = model.predict(X_valid)\n",
        "\n",
        "elif best_model_name == 'Neural Network':\n",
        "    nn_data = load_checkpoint('neural_network.pkl')\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = DemandNN(nn_data['input_size']).to(device)\n",
        "    model.load_state_dict(nn_data['model_state'])\n",
        "    model.eval()\n",
        "    X_valid_scaled = scaler.transform(X_valid)\n",
        "    with torch.no_grad():\n",
        "        X_valid_tensor = torch.FloatTensor(X_valid_scaled).to(device)\n",
        "        predictions = model(X_valid_tensor).cpu().numpy().flatten()\n",
        "\n",
        "print(f\"Predictions generated: {len(predictions)} samples\")\n",
        "print(f\"Prediction range: ${predictions.min():,.0f} - ${predictions.max():,.0f}\")\n",
        "print(f\"Average prediction: ${predictions.mean():,.0f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBZSiVqStDsr"
      },
      "source": [
        "## Saving Results & Submission File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "hn3yYypRtDsr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9bad22d-6043-43cf-c5c5-872441a3759c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to: /content/drive/MyDrive/demand_predictions.csv\n",
            "Checkpoint saved: final_predictions.pkl\n",
            "Best Model: Random Forest\n",
            "Best RMSE: $12,261.98\n",
            "All checkpoints saved to: /content/drive/MyDrive/demand_prediction_checkpoints/\n",
            "Predictions saved to: /content/drive/MyDrive/demand_predictions.csv\n"
          ]
        }
      ],
      "source": [
        "submission_df = pd.DataFrame({\n",
        "    'SalesID': sales_ids,\n",
        "    'SalePrice': predictions\n",
        "})\n",
        "\n",
        "# Save to CSV\n",
        "submission_path = '/content/drive/MyDrive/demand_predictions.csv'\n",
        "submission_df.to_csv(submission_path, index=False)\n",
        "print(f\"Predictions saved to: {submission_path}\")\n",
        "save_checkpoint(submission_df, 'final_predictions.pkl')\n",
        "\n",
        "summary = {\n",
        "    'model_results': results_df.to_dict('records'),\n",
        "    'best_model': best_model_name,\n",
        "    'best_metrics': best_model_info['best_metrics'],\n",
        "    'feature_names': list(X_train.columns),\n",
        "    'data_shapes': {\n",
        "        'train': X_train.shape,\n",
        "        'test': X_test.shape,\n",
        "        'validation': X_valid.shape\n",
        "    },\n",
        "    'prediction_summary': {\n",
        "        'count': len(predictions),\n",
        "        'mean': float(predictions.mean()),\n",
        "        'std': float(predictions.std()),\n",
        "        'min': float(predictions.min()),\n",
        "        'max': float(predictions.max())\n",
        "    },\n",
        "    'timestamp': datetime.now().isoformat()\n",
        "}\n",
        "\n",
        "\n",
        "print(f\"Best Model: {best_model_name}\")\n",
        "print(f\"Best RMSE: ${best_model_info['best_metrics']['rmse']:,.2f}\")\n",
        "print(f\"All checkpoints saved to: {checkpoint_dir}\")\n",
        "print(f\"Predictions saved to: {submission_path}\")"
      ]
    }
  ]
}